---
title: Ilya NIPS2024 Talk
date: '2024-12-14 15:59:12'
math: false
excerpt: ''
tags:
- 科研
- input
---

Seq2Seq获得NIPS2024时间检验奖，所以Ilya发表了一个Talk： [Ilya最新演讲:AI接下来是超级智能\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1Q8BGYsEHy)

Ilya的几个有用的观点
1. Seq2Seq是第一个在大规模数据集上训练的大参数量自回归神经模型。它的核心就是相信足够多数据足够大模型，自回归训练能学到序列上的任何分布
2. 当时Seq2Seq有个DeepLearning核心假设，就是人类神经元跟人工神经元一样工作，人类0.1s内处理的感知信息，足够宽的10层神经网络也能处理。
3. ScalingLaws跟生物学规律很像：对数坐标下，生物体身体越大，大脑质量越大
4. 自GPT2以来的预训练范式即将到头了，原因是人类可以获取的互联网数据已经到头了，这就像化石能源一样不可再生
	1. 【当前对数据的利用还是太过粗放，以后肯定要对数据做分馏、精馏，再用到不同作用不同需求的模型中去】
5. Superintelligence应该有的几个特点：agentic具有能动性，reasons能推理，understands能从有限数据理解规则，is self aware具有自我意识
6. **一个系统如果做的推理越多，那就越难以预测**，就像人类预测不好Alpha Go
	1. 【就算使用相同规则，它的逻辑推理路径可能跟人类不一样】
7. 当前DeepLearning系统做的事情比较容易预测，因为它的目的本质上只是学习人类的直觉，也就是在0.1s内能产生的第一反应（可能就是来自于大量记忆的学习）
	1. 【我认为本质上深度学习反向传播确实是根据大量记忆数据让推理结果接近真实结果，但是它是一个独立的原始的学习系统，不能跟大量已有知识去互动、叠加、推理、顿悟】
8. 模型泛化能力远远不如人，但是也能做到一些OOD泛化，我们现在觉得它泛化能力不行的原因是因为我们对于模型的要求水涨船高。