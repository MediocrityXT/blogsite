

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blogsite/img/apple-touch-icon.png">
  <link rel="icon" href="/blogsite/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f85e5">
  <meta name="description" content="期末考试前复习数据挖掘与大数据分析课程，做一个笔记总结，也以便以后查阅。">
  <meta name="author" content="XT">
  <meta name="keywords" content="">
  
  <title>数据挖掘 - SummerSecret</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/blogsite/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/blogsite/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"mediocrityxt.github.io","root":"/blogsite/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading2.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/blogsite/js/utils.js" ></script>
  <script  src="/blogsite/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/blogsite/">&nbsp;<strong>SummerSecret</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blogsite/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blogsite/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blogsite/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blogsite/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blogsite/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/blogsite/img/banner.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="数据挖掘">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-08-04 16:25" pubdate>
        2020年8月4日 下午
      </time>
      
    </span>
  
</div>

<div class="mt-1">
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      7.7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      87
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">数据挖掘</h1>
            
            <div class="markdown-body">
              <p>期末考试前复习<code>数据挖掘与大数据分析</code>课程，做一个笔记总结，也以便以后查阅。</p>
<span id="more"></span>
<h1 id="Chapter-1-简介"><a href="#Chapter-1-简介" class="headerlink" title="Chapter.1 简介"></a>Chapter.1 简介</h1><p><a target="_blank" rel="noopener" href="https://ke.qq.com/webcourse/index.html#cid=968590&amp;term_id=101064256&amp;taid=32603282&amp;lite=1&amp;vid=5285890804313344045">https://ke.qq.com/webcourse/index.html#cid=968590&amp;term_id=101064256&amp;taid=32603282&amp;lite=1&amp;vid=5285890804313344045</a></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="什么是大数据？"><a href="#什么是大数据？" class="headerlink" title="什么是大数据？"></a>什么是大数据？</h3><p>大数据指，数据量规模巨大到无法通过传统数据库和软件技术在合理时间内处理、并整理成为人类所能解读的信息的，巨量的结构化或非结构化的数据。</p>
<h3 id="什么是数据挖掘？"><a href="#什么是数据挖掘？" class="headerlink" title="什么是数据挖掘？"></a>什么是数据挖掘？</h3><p>广义上指，从大量数据中挖掘有趣模式和知识的过程。</p>
<h2 id="大数据4V特征"><a href="#大数据4V特征" class="headerlink" title="大数据4V特征"></a>大数据4V特征</h2><ul>
<li><strong>Volume</strong> 数据规模大</li>
<li><strong>Velocity</strong> 数据流分析要求输入输出速度快</li>
<li><strong>Variety</strong> 数据有很多不同形式</li>
<li><strong>Veracity</strong> 数据的不准确性 [需要对数据的来源和类型进行验证，需要处理数据中的噪声和异常点，还要考虑到数据的暂时性，等等综合因素影响到数据的准确性和可信度]</li>
</ul>
<h2 id="数挖主要任务"><a href="#数挖主要任务" class="headerlink" title="数挖主要任务"></a>数挖主要任务</h2><ol>
<li>分类</li>
<li>聚类</li>
<li>关联规则挖掘</li>
<li>离群点检测</li>
</ol>
<h2 id="KDD过程"><a href="#KDD过程" class="headerlink" title="KDD过程"></a>KDD过程</h2><p><strong><em>数据挖掘是KDD的核心！</em></strong></p>
<p><img src="kdd.png" srcset="/blogsite/img/loading2.gif" lazyload alt="kdd"></p>
<h1 id="Chapter-2-认识数据以及数据预处理"><a href="#Chapter-2-认识数据以及数据预处理" class="headerlink" title="Chapter.2 认识数据以及数据预处理"></a>Chapter.2 认识数据以及数据预处理</h1><h2 id="属性类型"><a href="#属性类型" class="headerlink" title="属性类型"></a>属性类型</h2><h3 id="分类型（Categorical）"><a href="#分类型（Categorical）" class="headerlink" title="分类型（Categorical）"></a>分类型（Categorical）</h3><p><strong>标称（Nominal）-（特殊：二元）</strong><br>例: ID 号、眼球颜色、邮政编码<br><strong>序数（ Ordinal ）</strong><br>例: 军阶 、 GPA、用 {tall, medium, short}表示的高</p>
<p>此两者一般为离散的。</p>
<h3 id="数值型（Numerical）"><a href="#数值型（Numerical）" class="headerlink" title="数值型（Numerical）"></a>数值型（Numerical）</h3><p><strong>区间（Interval）</strong><br>例: 日历、摄氏或华氏温度.<br><strong>比率（Ratio）</strong><br>例: 开氏温度、长度、计数</p>
<p>此两者一般为连续的。</p>
<hr>
<h3 id="离散属性-Discrete-Attribute"><a href="#离散属性-Discrete-Attribute" class="headerlink" title="离散属性(Discrete Attribute)"></a>离散属性(Discrete Attribute)</h3><p><strong>有限或无限可数个值</strong><br>例: 邮政编码、计数、文档集的词</p>
<p><strong>常表示为整数变量或字符串变量</strong>   </p>
<h3 id="连续属性-Continuous-Attribute"><a href="#连续属性-Continuous-Attribute" class="headerlink" title="连续属性(Continuous Attribute)"></a>连续属性(Continuous Attribute)</h3><p><strong>属性值为实数</strong><br>例: 温度、高度、重量</p>
<p><strong>实践中, 实数只能用有限位数字的数度量和表示.</strong></p>
<p><strong>连续属性一般用浮点变量表示</strong></p>
<hr>
<p><strong>二元属性(binary attributes)</strong></p>
<p>离散属性的特例<br>仅取两个不同值，常用0、1表示</p>
<h3 id="对称的二元属性"><a href="#对称的二元属性" class="headerlink" title="对称的二元属性"></a>对称的二元属性</h3><p>两个值有同等重要性</p>
<p>例：性别</p>
<h3 id="非对称的二元属性-重要"><a href="#非对称的二元属性-重要" class="headerlink" title="非对称的二元属性[重要!]"></a>非对称的二元属性[重要!]</h3><p>通常，一个值比另一个更重要<br><strong>重要的值通常比较少出现，通常用1表示</strong><br>例如，化验结果{阴性，阳性}，其中阳性较少，但更值得关注</p>
<h2 id="数据的统计描述"><a href="#数据的统计描述" class="headerlink" title="数据的统计描述"></a>数据的统计描述</h2><h3 id="中心性"><a href="#中心性" class="headerlink" title="中心性"></a>中心性</h3><ul>
<li><p>均值(mean)</p>
</li>
<li><p>众数(mode)</p>
<p>一个数据集中可能有多个众数</p>
</li>
<li><p>中位数(median)</p>
<p>近似值估计(线性插值方法)：</p>
<p><img src="image-20200804232835061.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
</li>
<li><p>中列数(midrange)</p>
<p><strong>数据集的最大和最小值的平均值</strong></p>
</li>
</ul>
<h3 id="散度"><a href="#散度" class="headerlink" title="散度"></a>散度</h3><ul>
<li><p>极差：max-min</p>
</li>
<li><p>四分位数(Quantile)：3个数将数据分成四等分</p>
</li>
<li><p>四分位数极差：Q3-Q1的距离</p>
</li>
<li><p>方差</p>
</li>
<li><p>标准差</p>
</li>
<li><p>五数概括：[min,Q1,median,Q3,max]</p>
<p>可以用<strong>盒图(boxplot)</strong>表示</p>
</li>
</ul>
<h2 id="相似性度量"><a href="#相似性度量" class="headerlink" title="相似性度量"></a>相似性度量</h2><h3 id="标称属性"><a href="#标称属性" class="headerlink" title="标称属性"></a>标称属性</h3><p>i与j两个对象相异度度量方法:</p>
<script type="math/tex; mode=display">
\begin{gathered}
d(i,j)=\frac{p-m}{p}\\\\
m：状态相同的变量数目,p：变量总数
\end{gathered}</script><p><strong>计算二元变量的相似度：</strong></p>
<p>首先获取列联表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>对象j</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>1</td>
<td>0</td>
<td>sum</td>
</tr>
<tr>
<td><strong>对象i</strong></td>
<td>1</td>
<td>q</td>
<td>r</td>
<td>q+r</td>
</tr>
<tr>
<td></td>
<td>0</td>
<td>s</td>
<td>t</td>
<td>s+t</td>
</tr>
<tr>
<td></td>
<td>sum</td>
<td>q+s</td>
<td>r+t</td>
<td>p</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>对称:</strong> 二元变量的两个状态具有同等价值</p>
<p><strong>不对称</strong>: 二元变量的两个状态的输出不是同样重要</p>
</blockquote>
<p><strong>对称</strong>二元变量的相异度计算——简单匹配：</p>
<script type="math/tex; mode=display">
d(i,j)=\frac{r+s}{q+r+s+t}</script><p><strong>不对称</strong>二元变量的相异度计算：</p>
<blockquote>
<p><strong><em>对于非对称二元变量，负匹配数目t被忽略，即分母中删去0与0的匹配数。</em></strong></p>
</blockquote>
<script type="math/tex; mode=display">
\begin{gathered}
d(i,j)=\frac{r+s}{q+r+s}=1-\frac{q}{q+r+s}=1-Jaccard(i,j)\\\\
Jaccard系数用于比较有限样本集之间的相似性，该系数越大，相似度越高。
\end{gathered}</script><h3 id="数值属性"><a href="#数值属性" class="headerlink" title="数值属性"></a>数值属性</h3><p>常使用<strong>距离</strong>来度量两个数据对象之间的相异性</p>
<p><strong>闵可夫斯基(Minkowski)距离</strong>：</p>
<script type="math/tex; mode=display">
\begin{gathered}
d(i,j)=\sqrt[q]{|x_{i1}-x_{j1}|^{q}+|x_{i2}-x_{j2}|^{q}+\cdots+|x_{ip}-x_{jp}|^{q}}\\\\
其中i和j是两个p-维数据对象(q为正整数)
\end{gathered}</script><p><strong>曼哈顿(Manhattan)距离</strong>：上式中q=1</p>
<p><strong>欧几里德(Euclidean)距离</strong>：上式中q=2</p>
<p>另外距离也可以加权计算。</p>
<h3 id="数据标准化-归一化"><a href="#数据标准化-归一化" class="headerlink" title="数据标准化/归一化"></a>数据标准化/归一化</h3><h4 id="数据标准化："><a href="#数据标准化：" class="headerlink" title="数据标准化："></a>数据标准化：</h4><ol>
<li><p>计算平均绝对偏差</p>
<script type="math/tex; mode=display">
\begin{gathered}
s_{f}=\frac{1}{n}(|x_{1f}-m_{f}|+|x_{2f}-m_{f}|+\cdots+|x_{nf}-m_{f}|)\\\\
其中m_{f}=\frac{1}{n}(x_{1f}+x_{2f}+\cdots+x_{nf})
\end{gathered}</script></li>
<li><p>计算标准化的度量值</p>
<script type="math/tex; mode=display">
Z_{if}=\frac{x_{if}-m_{f}}{s_{f}}</script></li>
</ol>
<p>使用<strong>平均绝对偏差</strong>比使用<strong>标准差</strong>更具有鲁棒性</p>
<h4 id="数据归一化-！！-："><a href="#数据归一化-！！-：" class="headerlink" title="数据归一化[！！]："></a>数据归一化[！！]：</h4><p>计算混合类型变量描述的对象的相异度的基本思想是，将不同类型的变量组合在单个相异度矩阵中, <strong>把所有变量转换到共同的值域区间[0.0, 1.0]上</strong> 。</p>
<ul>
<li>最大最小法</li>
</ul>
<script type="math/tex; mode=display">
x'=\frac{x-x_{min}}{x_{max}-x_{min}}</script><ul>
<li>z-score</li>
</ul>
<script type="math/tex; mode=display">
x'=\frac{x-\mu}{\sigma}</script><h3 id="其他相似性度量方式"><a href="#其他相似性度量方式" class="headerlink" title="其他相似性度量方式"></a>其他相似性度量方式</h3><p>余弦相似性（向量内积空间的夹角）<br>马氏距离 （考虑数据局部分布）<br>相关系数 （<a href="#相关分析：计算相关系数-皮尔逊系数">皮尔森系数</a>）<br>KL散度（数据分布比较） </p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="数据清理-！！"><a href="#数据清理-！！" class="headerlink" title="数据清理[！！]"></a>数据清理[！！]</h3><ul>
<li>空缺值。<strong>用平均值或相近的样本平均值或用空值替代空缺值。</strong></li>
<li>处理噪声数据，删除孤立点。可用分箱、聚类、回归等方法处理噪声。</li>
</ul>
<h3 id="数据集成"><a href="#数据集成" class="headerlink" title="数据集成"></a>数据集成</h3><ul>
<li>冗余数据分析<ul>
<li>数值型：相关分析</li>
<li>标称型：卡方检验</li>
</ul>
</li>
<li>集成多个数据库、数据立方体或文件</li>
</ul>
<h4 id="相关分析：计算相关系数-皮尔逊系数"><a href="#相关分析：计算相关系数-皮尔逊系数" class="headerlink" title="相关分析：计算相关系数(皮尔逊系数)"></a>相关分析：计算相关系数(皮尔逊系数)</h4><script type="math/tex; mode=display">
\begin{align}
r_{A,B}&=\frac{\sum_{i=1}^{n}(a_{i}-\overline{A})(b_{i}-\overline{B})}{(n-1)\sigma_{A}\sigma_{B}}\\
&=\frac{\sum_{i=1}^{n}(a_{i}b_{i})-n\overline{A}\overline{B}}{(n-1)\sigma_{A}\sigma_{B}}\\\\
\sigma_{A}，\sigma_{B}&为各自标准差,\sum_{i=1}^{n}(a_{i}b_{i})=A\cdot B
\end{align}\\</script><p>r=0不相关，r&gt;0正相关，r&lt;0负相关</p>
<h4 id="卡方检验-！！重要！！"><a href="#卡方检验-！！重要！！" class="headerlink" title="卡方检验[！！重要！！]"></a><strong>卡方检验[！！重要！！]</strong></h4><p>σij是(ai,bj)的观测频度（实际计数）<br>eij是(ai,bj)的期望频度（单位是次数）<br>N是数据元组的个数</p>
<script type="math/tex; mode=display">
\begin{gathered}
\chi^2=\sum_{i=1}^c\sum_{j=1}^r\frac{(\sigma_{ij}-e_{ij})^2}{e_{ij}}\\\\
e_{ij}=\frac{count(A=a_{i})*count(b=b_{j})}{N}\\\\
自由度:(c-1)*(r-1)\\\\
\chi^2越大，两者无关系的概率越小，有关系的概率越大
\end{gathered}</script><p><img src="chi-square.png" srcset="/blogsite/img/loading2.gif" lazyload alt="chi-square"></p>
<h3 id="数据归约"><a href="#数据归约" class="headerlink" title="数据归约"></a>数据归约</h3><ul>
<li>得到数据集的压缩表示，但可以得到相同或相近的结果</li>
<li>数据归约策略：<ul>
<li>维归约:小波分析、PCA、特征筛选</li>
<li>数量归约：回归、聚类、采样、数据立方体聚集</li>
<li>数据压缩：使用变换</li>
</ul>
</li>
</ul>
<hr>
<p><strong>小波变换:</strong></p>
<p>保存小波较大的系数进行原始数据的压缩，主要用于图像分析中。</p>
<p><strong>PCA——Principal component analysis（主成分分析），K-L变换:</strong></p>
<p>基本思想是找到一个投影，其能表示数据的最大变化</p>
<h4 id="特征筛选-！！-用到信息熵"><a href="#特征筛选-！！-用到信息熵" class="headerlink" title="特征筛选[！！] 用到信息熵"></a>特征筛选[！！] 用到信息熵</h4><p><a name='信息熵'></a></p>
<p><strong>信息熵</strong>：刻画系统的混乱程度</p>
<script type="math/tex; mode=display">
\begin{gathered}
H(X)=-\sum_{i=1}^np(x_{i})\log p(x_{i})\\\\
p(x_i)为X出现x_i的概率
\end{gathered}</script><p>条件信息熵：在已知X的基础上需要多少信息来描述Y</p>
<script type="math/tex; mode=display">
H(Y|X)=\sum_{x\in\chi}p(x)H(Y|X=x)</script><p><strong>信息增益</strong>：刻画在已知X的基础上需要节约多少信息来描述Y</p>
<script type="math/tex; mode=display">
IG(Y|X)=H(Y)-H(Y|X)</script><p>特征筛选即<strong>选择对分类变量Y信息增益大的特征</strong>，删去信息增益小的特征。</p>
<h3 id="数据变换"><a href="#数据变换" class="headerlink" title="数据变换"></a>数据变换</h3><ul>
<li>规范化（归一化）和聚集</li>
</ul>
<h3 id="数据离散化"><a href="#数据离散化" class="headerlink" title="数据离散化"></a>数据离散化</h3><ul>
<li>将连续数据进行离散处理<ul>
<li>通过将属性域划分为区间，减少给定连续属性值的个数。区间的标号可以代替实际的数据值。</li>
</ul>
</li>
<li>用高层概念代替底层属性值<ul>
<li>通过使用高层的概念（比如：青年、中年、老年）来替代底层的属性值（比如：实际的年龄数据值）来规约数据。</li>
</ul>
</li>
</ul>
<h1 id="Chapter-3-关联规则挖掘"><a href="#Chapter-3-关联规则挖掘" class="headerlink" title="Chapter.3 关联规则挖掘"></a>Chapter.3 关联规则挖掘</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ul>
<li><p>关联规则挖掘</p>
<ul>
<li>产生频繁项集：发现满足最小支持度阈值的所有项集，这些项集称作频繁项集。</li>
<li>产生规则：从上一步发现的频繁项集中提取<strong>所有高置信度</strong>的规则，这些规则称作<strong>强规则</strong>（strong rule）。</li>
</ul>
</li>
<li><p>频繁模式：数据库中频繁出现的项集。</p>
</li>
<li><p>项集(Itemset)：包含0个或多个项的集合。包含k个项的项集称为k-项集。</p>
</li>
<li><p>支持度计数(Support Count)：包含特定项集的事务个数。</p>
</li>
<li><p>支持度(Support)：包含项集的事务数与总事务数的比值。</p>
</li>
<li><p>关联规则：关联规则是形如 X→Y的蕴含表达式, 其中 X 和 Y 是不相交的项集<br>例子:    {Milk, Diaper} → {Beer} </p>
<ul>
<li>支持度 support (X→Y): 确定<strong>项集的频繁程度</strong>。<script type="math/tex; mode=display">
\begin{gathered}
support(X\rightarrow Y)=P(X\cup Y)\\\\
P(X\cup Y) 表示事务包含集合A和B中每个项的概率 
\end{gathered}</script></li>
</ul>
</li>
<li><p>置信度confidence(X→Y)：确定<strong>Y在包含X的事务中出现的频繁程度</strong>。</p>
<script type="math/tex; mode=display">
  confidence(X\rightarrow Y)=P(X|Y)=\frac{support(X\cup Y)}{support(X)}</script></li>
</ul>
<h2 id="Apriori算法"><a href="#Apriori算法" class="headerlink" title="Apriori算法"></a>Apriori算法</h2><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p><strong>如果一个项集是频繁的，则它的所有子集一定也是频繁的。相反，如果一个项集是非频繁的，则它的所有超集也一定是非频繁的</strong>。</p>
<blockquote>
<p>这种基于支持度度量修剪指数搜索空间的策略称为<strong>基于支持度的剪枝</strong>（support-based pruning）。</p>
<p>这种剪枝策略依赖于支持度度量的一个关键性质，即一个项集的支持度决不会超过它的子集的支持度。这个性质也称为<strong>支持度度量的反单调性</strong>（anti-monotone）。</p>
</blockquote>
<h3 id="Apriori流程和计算-！！！！"><a href="#Apriori流程和计算-！！！！" class="headerlink" title="Apriori流程和计算[！！！！]"></a>Apriori流程和计算[！！！！]</h3><p>由长度为k的频繁项集（通过连接，剪枝）产生长度为 (k+1) 的候选项集, 并且根据 DB扫描筛选这些候选。</p>
<ol>
<li>连接：将L<sub>k</sub>（频繁k-项集的集合）中<strong>所有项随机组合</strong>，得到所有可能的k+1项集的集合。</li>
<li>剪枝：将连接得到的集合中<strong>含有不频繁子集的(k+1)-项集删去</strong>，得到候选k+1项集的集合。</li>
<li>扫描：计算所有候选项集的支持度技术，<strong>将小于最小支持度计数的项集删去</strong>，得到频繁k+1项集的集合。</li>
</ol>
<h3 id="找到强关联规则-！！"><a href="#找到强关联规则-！！" class="headerlink" title="找到强关联规则[！！]"></a>找到强关联规则[！！]</h3><p>将频繁项集的集合Y划分成两个非空子集X和Y-X，使得X→Y-X满足置信度阈值。</p>
<p>例：</p>
<blockquote>
<p>如果 {A,B,C,D} 是频繁项集, 候选项集为:</p>
<p>ABC →D,     ABD →C,     ACD →B,     BCD →A, A →BCD,    B →ACD,    C → ABD,     D →ABC，AB →CD,    AC → BD,     AD → BC,     BC →AD, BD →AC,     CD →AB    </p>
<p>一般，计算关联规则的置信度并不需要再次扫描事务数据集。</p>
<p><strong>规则{A,B,C} →{D}的置信度为support(ABCD)/ support (ABC)。</strong></p>
</blockquote>
<h3 id="挑战和改进"><a href="#挑战和改进" class="headerlink" title="挑战和改进"></a>挑战和改进</h3><ul>
<li>挑战<ul>
<li>事务数据库的多遍扫描</li>
<li>数量巨大的候选</li>
<li>候选支持度计数繁重的工作量</li>
</ul>
</li>
<li>改进Apriori的基本思想<ul>
<li>减少事务数据库的扫描遍数</li>
<li>压缩候选数量</li>
<li>便于候选计数</li>
</ul>
</li>
<li>方法<ul>
<li>哈希散列项集计数——压缩候选k项集</li>
<li>事务压缩——删除不可能对寻找频繁项集有用的事务</li>
<li>划分——分而治之【项集在DB中是频繁的, 它必须至少在DB的一个划分中是频繁的】</li>
<li>抽样——选取原数据库的一个样本, 使用Apriori 算法在样本中挖掘频繁模式</li>
</ul>
</li>
</ul>
<h2 id="FP增长算法-Frequent-Pattern-Growth"><a href="#FP增长算法-Frequent-Pattern-Growth" class="headerlink" title="FP增长算法(Frequent-Pattern Growth)"></a>FP增长算法(Frequent-Pattern Growth)</h2><h3 id="如何构造FP树"><a href="#如何构造FP树" class="headerlink" title="如何构造FP树"></a>如何构造FP树</h3><ol>
<li>支持度排序：扫描一次数据集，确定每个项的支持度计数。而将频繁项按照支持度的递减排序(<strong>为了给每个{a,b,c…}内部的字母排序</strong>)。L = {a:8, b:7, c:6, d:5, e:3} </li>
<li>构建FP树：第二次扫描数据集，在处理每个事务时无视(删去)其中的非频繁项，例：<ol>
<li>读入第一个事务{a, b}之后，创建标记为a和b的结点。然后形成null-&gt;a-&gt;b路径。该路径上的所有结点的频度计数为1。</li>
<li>读入第二个事务{b,c,d}之后，为项b,c和d创建新的结点集。然后，连接结点null-&gt;b-&gt;c-&gt;d，形成一条代表该事务的路径。该路径上的每个结点的频度计数也等于1。</li>
<li>第三个事务{a，c，d，e}与第一个事务共享一个共同的前缀项a，所以第三个事务的路径null-&gt;a-&gt;c-&gt;d-&gt;e与第一个事务的路径null-&gt;a-&gt;b部分重叠。因为它们的路径重叠，所以结点a的频度计数增加为2。</li>
<li>继续该过程，直到每个事务都映射到FP树的一条路径。</li>
</ol>
</li>
</ol>
<blockquote>
<p>注意：尽管前两个事务具有一个共同项b，但是它们的路径不相交，因为这两个事务没有共同的前缀</p>
</blockquote>
<p><img src="image-20200807133204031.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<p><img src="image-20200807133307221.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<h3 id="如何挖掘频繁模式-！！"><a href="#如何挖掘频繁模式-！！" class="headerlink" title="如何挖掘频繁模式[！！]"></a>如何挖掘频繁模式[！！]</h3><ol>
<li><p>对于头表中每项构建条件模式基（一个“子数据库”，由FP树中与该后缀模式一起出现的前缀路径集组成）</p>
</li>
<li><p>构建条件FP树（根据对应项头表，将条件模式基看作为事务数据库，构造每一个频繁项的条件FP树，且删去了每棵树的非频繁节点）</p>
</li>
<li><p>挖掘频繁项集</p>
<ol>
<li>如果该项的条件FP树为单个路径，则<strong>包含该项</strong>的频繁项集由该项与路径下所有节点组合。</li>
<li>如果该项的条件FP树为多路径，则针对树包含的每个项<strong>再次迭代</strong>，挖掘<strong>包含该项</strong>的频繁项集（即迭代得到的频繁项集加上该项）。</li>
</ol>
</li>
</ol>
<h3 id="两算法区别"><a href="#两算法区别" class="headerlink" title="两算法区别"></a>两算法区别</h3><ol>
<li>范式不同。产生、测试得到频繁项集 vs 在FP树上直接提取。</li>
<li>时间复杂度不同。FP增长算法比Apriori快了一个数量级。</li>
<li>内存占用不同。FP树使用内存较多。</li>
<li>FP更复杂，实现难度更大。</li>
</ol>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>支持度、置信度、兴趣因子</p>
<h1 id="Chapter-4-分类"><a href="#Chapter-4-分类" class="headerlink" title="Chapter.4 分类"></a>Chapter.4 分类</h1><h2 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="监督-无监督"><a href="#监督-无监督" class="headerlink" title="监督/无监督"></a>监督/无监督</h3><p><strong>监督学习（分类/预测）</strong><br>数据集中对象的类标记已知<br>通过类标记的指导下学习数据中的模式<br>利用获取的模式或者模型对新数据进行分类预测</p>
<p><strong>无监督的学习（关联规则，聚类分析）</strong><br>数据集中对象的类标记（概念）是未知的<br>挖掘潜在的数据内部模式</p>
<h3 id="生成-判别"><a href="#生成-判别" class="headerlink" title="生成/判别"></a>生成/判别</h3><p><strong>生成模型（Generative Model）</strong><br>希望从数据中学习/还原出原始的真实数据生成模型。常见的方法是学习数据的联合概率分布。E.g 朴素贝叶斯方法、隐马尔科夫模型等。</p>
<p><strong>判别模型 (Discriminative Model)</strong><br>从数据中学习到不同类概念的区别从而进行分类。如KNN，SVM, ANN, Decision Tree, etc.</p>
<h3 id="分类-回归"><a href="#分类-回归" class="headerlink" title="分类/回归"></a>分类/回归</h3><p><strong>分类</strong></p>
<p>根据训练数据集和类标号属性，构建模型来分类现有数据和分类新数据</p>
<p><strong>预测/回归</strong></p>
<p>建立连续函数值模型，预测连续值或趋势比如预测空缺值</p>
<h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树[!]"></a>决策树[!]</h3><p>构造过程——决策归纳树算法（是一个贪心算法）</p>
<p>树以代表训练样本的单个结点开始。<br>如果样本都在同一个类，则该结点成为树叶，并用该类标记。<br>否则，算法选择最有分类能力的属性作为决策树的当前结点。<br>根据当前决策结点属性取值的不同，将训练样本数据集分为若干子集，每个取值形成一个分枝，有几个取值形成几个分枝。针对上一步得到的一个子集，重复进行先前步骤，递归形成每个划分样本上的决策树。一旦一个属性出现在一个结点上，就不必在该结点的任何后代考虑它。<br>递归划分步骤仅当下列条件之一成立时停止：<br>①给定结点的所有样本属于同一类。<br>②没有剩余属性可以用来进一步划分样本。<br>③如果某一分枝，没有满足该分支中已有分类的样本，则以样本的多数类创建一个树叶。</p>
<h3 id="属性选择"><a href="#属性选择" class="headerlink" title="属性选择"></a>属性选择</h3><h4 id="属性选择度量"><a href="#属性选择度量" class="headerlink" title="属性选择度量"></a>属性选择度量</h4><p>又称分裂规则，决定给定节点上的元组如何分裂<br>具有最好度量得分的属性（对分出的数据类别越“纯”）选定为分裂属性</p>
<h4 id="三种度量"><a href="#三种度量" class="headerlink" title="三种度量"></a>三种度量</h4><ul>
<li>信息增益</li>
<li>信息增益率</li>
<li>Gini指标</li>
</ul>
<p><a href="#信息熵">信息增益</a></p>
<p>信息增益率</p>
<script type="math/tex; mode=display">
\begin{gathered}
Info(D)=-\sum_{i=1}^mp_i\log _2(p_i)\\\\
Info_A(D)=\sum_{j=1}^v\frac{|D_{j}|}{|D|}\times Info(D)\\\\
(用A分裂D得到v个部分需要的信息)\\\\
Gain_{A}(D)=Info(D)-Info_A(D)\\\\
(信息增益)\\\\
SplitInfo_{A}(D)=-\sum_{j=1}^v\frac{|D_{j}|}{|D|}\times\log_{2}(\frac{|D_{j}|}{|D|})\\\\
GainRatio_{A}=\frac{Gain_{A}(D)}{SplitInfo_{A}(D)}\\\\
(增益率)
\end{gathered}</script><p><img src="image-20200807195604165.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><h4 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h4><p>选择与待分类样本距离最小的K个样本作为X的K个最近邻，最后以X的K个最近邻中的大多数所属的类别作为X的类别。</p>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>算距离：给定测试对象，计算它与训练集中的每个对象的距离；<br>找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻；<br>做分类：根据这k个近邻归属的主要类别，来对测试对象分类。</p>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>简单。适合多标签问题。</p>
<p>懒惰学习(收到样本后保存整个训练集，预测时才计算，而非用训练集提前算出模型)算法计算量大，内存开销大。样本不平衡时效果不好。可解释性差。</p>
<h3 id="朴素贝叶斯分类"><a href="#朴素贝叶斯分类" class="headerlink" title="朴素贝叶斯分类"></a>朴素贝叶斯分类</h3><p><strong>计算</strong></p>
<script type="math/tex; mode=display">
\begin{gather}
P(H|X)=\frac{P(X|H)P(H)}{P(X)}\\\\
[H后验概率=X似然概率\times H先验概率\div X先验概率]
\end{gather}</script><p>将有最高后验概率的类作为预测结果，</p>
<p>即让P(X|C<sub>i </sub>)P(C<sub>i </sub>)最大化。</p>
<p><strong>假设</strong></p>
<p>类条件独立：</p>
<p>给定元组的类标号，假定属性值有条件地相互独立（即属性间不存在依赖关系）</p>
<h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>追求结构风险最小化</p>
<p>支持向量/小样本 </p>
<p>最大间隔化思想-有泛化能力</p>
<h4 id="基本思想-2"><a href="#基本思想-2" class="headerlink" title="基本思想"></a>基本思想</h4><p><img src="image-20200807201347022.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<p>分类面H的表达式：wx+b=0</p>
<p>支持向量为两分界面Plus-Plane(H1)\Minus-Plane(H2)上的样本点对应向量。</p>
<p>线性可分：<br>求解使得超平面具有最大内间间隔的w，b参数；（<strong>条件最优化问题！！！</strong>）</p>
<script type="math/tex; mode=display">
\begin{gather}
\min\limits_{w,b}\frac{1}{2}||w||^2 \\\\
s.t.     
y_i(wx_i+b)\ge 1\\\\
(乘以y_i相当于判断绝对值，将所有点分开)
\end{gather}</script><p>将问题转化为对偶问题进行快速求解；（凸二次规划问题利用Lagrange乘子法）</p>
<p>改进：加入松弛变量和惩罚因子的SVM</p>
<p>非线性问题：核函数</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>过拟合问题</p>
<ul>
<li>增加数据</li>
<li>删去异常点</li>
<li>增加正则项</li>
<li>Train-Validation-Test</li>
</ul>
<p>决策树中的过拟合</p>
<ul>
<li>限制层高</li>
<li><p>数少的节点不再分割</p>
</li>
<li><p>先剪枝/后剪枝</p>
</li>
</ul>
<p>Naive Bayes 贝叶斯理论</p>
<p>人工神经网络 感知机 BP网络</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="学习准则"><a href="#学习准则" class="headerlink" title="学习准则"></a>学习准则</h3><p>准确性</p>
<p>多样性：多个算法组合</p>
<h3 id="集成策略-！！"><a href="#集成策略-！！" class="headerlink" title="集成策略[！！]"></a>集成策略[！！]</h3><p>Bagging</p>
<p>Boosting</p>
<p>Stacking</p>
<h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p>准确率：识别率</p>
<p>精度：被分类器标记为正类的样本实际为“正类”的比例</p>
<p>召回率：正样本标记为正的百分比</p>
<p>F1：精度和召回率的调和平均值</p>
<p>当类不平衡时：灵敏度（正确识别的正样本的百分比） ， 特效性（正确识别的负样本的百分比）</p>
<h1 id="Chapter-5-聚类及离群点检测"><a href="#Chapter-5-聚类及离群点检测" class="headerlink" title="Chapter.5 聚类及离群点检测"></a>Chapter.5 聚类及离群点检测</h1><h2 id="什么是聚类"><a href="#什么是聚类" class="headerlink" title="什么是聚类"></a>什么是聚类</h2><p>就是将数据分为多个簇(Clusters)，使得在同一个簇内对象之间具有较高的相似度，而不同簇之间的对象差别较大。</p>
<ul>
<li><p>探索数据内部潜在的自然分组结构</p>
</li>
<li><p>无监督学习（无类别信号）</p>
</li>
</ul>
<h2 id="聚类算法分类-！！"><a href="#聚类算法分类-！！" class="headerlink" title="聚类算法分类[！！]"></a>聚类算法分类[！！]</h2><ul>
<li>划分方法（partitioning  method）<ul>
<li>K-means算法</li>
</ul>
</li>
<li>层次的方法（hierarchical method）<ul>
<li>AGNES算法（自底向上、凝聚算法）</li>
<li>DIANA算法（自顶向下、分裂算法）</li>
</ul>
</li>
<li>基于密度的方法（density-based method）<ul>
<li>DBSCAN算法</li>
</ul>
</li>
<li>基于网格的方法（grid-based method）<ul>
<li>STING算法</li>
</ul>
</li>
</ul>
<h3 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h3><p>属于划分方法</p>
<h4 id="目标："><a href="#目标：" class="headerlink" title="目标："></a>目标：</h4><p>将数据集D划分为K个互不相容的簇，使得簇内对象互相相似，簇间差异大。</p>
<h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><ol>
<li>设定K，随机选取K个初始值，作为簇中心点。</li>
<li>将所有对象分配到最近的簇中（即找到与之距离最小的簇中心点）。</li>
<li>重新计算每个簇的中心点，并计算所有点到其所属簇中心的距离之和E。</li>
<li>迭代。继续重新分配并计算E。</li>
<li>当E收敛不变时结束。</li>
</ol>
<h4 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h4><ul>
<li>优点：<ul>
<li>经典算法，简单、快速。</li>
<li>对处理大数据集，该算法是相对可伸缩和高效率的。</li>
</ul>
</li>
<li>缺点<ul>
<li>必须事先给出k（不是 parameter-free clustering algorithms）</li>
<li>对初值敏感，对于不同的初始值，可能会导致不同结果。</li>
<li>不适合于发现非球形状的簇或者大小差别很大的簇。（这适合使用DBSCAN）</li>
<li>对于“噪声”和孤立点数据是敏感的（ K-中心点）</li>
</ul>
</li>
</ul>
<h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><p>基于密度的聚类算法。</p>
<h4 id="思想："><a href="#思想：" class="headerlink" title="思想："></a>思想：</h4><p>对于一个类中的每个对象，在其给定半径的领域中包含的对象不能少于某一给定的最小数目<br>在DBSCAN中，发现一个类的过程是基于这样的事实：一个类能够被其中的任意一个核心对象所确定</p>
<h4 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h4><ol>
<li>对象的ε-临域（ Eps 近邻）：以该对象为圆心，半径ε内的区域。</li>
<li>核心对象：如果一个对象的ε-临域至少包含最小数目MinPts个对象，则称该对象为核心对象。</li>
<li>直接密度可达：给定一个对象集合D，如果p是在q的ε-邻域内，而q是一个核心对象，我们说对象p从对象q出发是直接密度可达的。（<strong>核心对象到其邻域内的对象都是直接密度可达的，单向关系</strong>）</li>
<li>密度可达的：如果存在一个对象链p1，p2，…，pn，p1=q，pn=p，对pi∈D，（1&lt;=i&lt;=n），pi+1是从pi关于ε和MitPts直接密度可达的，则对象p(pn)是从对象q(p1)关于ε和MinPts密度可达的。（<strong>对象链从p1到pn-1全为核心对象，且距离不得超过ε，这样才密度可达pn。pn可以不是核心对象</strong>）</li>
<li>密度相连的：如果对象集合D中存在一个对象o，使得对象p和q是从o关于ε和MinPts密度可达的，那么对象p和q是关于ε和MinPts密度相连的。（<strong>o到p密度可达，o到q密度可达，则p和q密度相连，中间经过对象全为核心对象。p,q可以不是核心对象</strong>）</li>
<li>噪声: 一个基于密度的簇是基于密度可达性的最大的密度相连对象的集合。<strong>不包含在任何簇中</strong>的对象被认为是“噪声”</li>
</ol>
<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><p>DBSCAN算法描述<br>算法5-5 DBSCAN  </p>
<p>输入：包含n个对象的数据库，半径ε，最少数目MinPts。<br>输出：所有生成的簇，达到密度要求。</p>
<ol>
<li>REPEAT</li>
<li>从数据库中抽取一个未处理过的点；</li>
<li>IF 抽出的点是核心点 THEN找出所有从该点密度可达的对象，形成一个簇</li>
<li>ELSE 抽出的点是边缘点(非核心对象)，跳出本次循环，寻找下一点；</li>
<li>UNTIL 所有点都被处理；</li>
</ol>
<h2 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h2><h3 id="什么是离群点"><a href="#什么是离群点" class="headerlink" title="什么是离群点"></a>什么是离群点</h3><blockquote>
<p>“离群点是一个数据对象，它显著不同于其它数据对象，好像它是被不同的机制产生的一样。”</p>
</blockquote>
<h3 id="离群点种类"><a href="#离群点种类" class="headerlink" title="离群点种类"></a>离群点种类</h3><ul>
<li>全局离群点</li>
<li>局部离群点</li>
<li>集体离群点</li>
</ul>
<h4 id="LOF"><a href="#LOF" class="headerlink" title="LOF"></a>LOF</h4><p>局部异常因子(Local Outlier Factor) </p>
<p>计算公式暂略</p>
<blockquote>
<p>n对象<em>p</em>的局部异常因子表示<em>p</em>的异常程度，局部异常因子愈大，就认为它更可能异常；反之则可能性小。</p>
<p>n簇内靠近核心点的对象的<em>LOF</em>接近于1，那么不应该被认为是局部异常。而处于簇的边缘或是簇的外面的对象的LOF相对较大，如前面图中对象o1， o2。 </p>
</blockquote>
<h1 id="Chapter-6-大数据分析"><a href="#Chapter-6-大数据分析" class="headerlink" title="Chapter.6 大数据分析"></a>Chapter.6 大数据分析</h1><h2 id="哈希技术"><a href="#哈希技术" class="headerlink" title="哈希技术"></a>哈希技术</h2><h3 id="整体框架"><a href="#整体框架" class="headerlink" title="整体框架"></a>整体框架</h3><p><img src="image-20200824102425400.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<h3 id="Shingling"><a href="#Shingling" class="headerlink" title="Shingling"></a>Shingling</h3><p>一个文档的 k-shingle (或 k-gram) 是指连续k个字符一起出现的序列。</p>
<blockquote>
<p>Example: k=2; doc = abcab.  Set of 2-shingles = {ab, bc, ca}.<br>Option: regard shingles as a bag, and count ab twice.</p>
</blockquote>
<p>一个文档可以通过k-shingles来很好表示.</p>
<h3 id="Min-Hash-最小哈希"><a href="#Min-Hash-最小哈希" class="headerlink" title="Min-Hash 最小哈希"></a>Min-Hash 最小哈希</h3><p>思想：</p>
<p>先给出n个随机序数列，用来代替原矩阵的行的序号（相当于n种矩阵换行的方式）。</p>
<p>定义哈希函数h(C)=第C列从上到下出现的第一个1（故称为“最小哈希”）的行序号在n个随机序数列中的对应数。</p>
<p>h(C)为[1,n]的列向量，在签名矩阵中位于第C列。</p>
<h4 id="计算签名矩阵-！！"><a href="#计算签名矩阵-！！" class="headerlink" title="计算签名矩阵[！！]"></a>计算签名矩阵[！！]</h4><p>实际实现时不用生成n个随机序数列，而是使用n个哈希函数（例如：h(r)=r mod 5)代替。</p>
<p>假设原数据矩阵为[X,Y]大小。</p>
<p>通过对C列从Row1到RowX的<strong>每个为1的元素的行号r</strong>依次计算多个哈希h(r)，找出并保留得到的对于C列来说每种哈希函数的最小哈希值。</p>
<p>得到一个签名矩阵[n,Y]，其中位于(x,y)的元素代表第y列的<strong>所有对应元素为1的行号</strong>在第x个哈希函数下计算得到的最小哈希值。</p>
<p>而这时</p>
<script type="math/tex; mode=display">
Sim(C_i,C_j)=Pr(h(C_i)=h(C_j))</script><h4 id="通过签名矩阵寻找相似的签名"><a href="#通过签名矩阵寻找相似的签名" class="headerlink" title="通过签名矩阵寻找相似的签名"></a>通过签名矩阵寻找相似的签名</h4><p>计算<strong>签名矩阵</strong>两列中相同数字占比。</p>
<p>签名矩阵中相似的两项在输入矩阵中也可能相似。</p>
<h4 id="检测签名相似的是否真的相似"><a href="#检测签名相似的是否真的相似" class="headerlink" title="检测签名相似的是否真的相似"></a>检测签名相似的是否真的相似</h4><p>【注意】计算<strong>输入矩阵</strong>中的两列二元非对称变量（0-1变量）的相似度Similarity：需要删去负匹配数目（0与0匹配）后，计算正匹配（1与1匹配）所占比例</p>
<h3 id="局部敏感哈希Locality-Sensitive-Hashing，LSH"><a href="#局部敏感哈希Locality-Sensitive-Hashing，LSH" class="headerlink" title="局部敏感哈希Locality-Sensitive Hashing，LSH"></a>局部敏感哈希Locality-Sensitive Hashing，LSH</h3><p>将签名矩阵按行分块(band)，比如r行1块，对于每一块中的每列计算哈希（哈希桶数目尽量多）。</p>
<p>如果这块中有某两列落在一个bucket中，说明这两列可能相同。</p>
<p>相似的两列，至少有1band会出现落在同个bucket的candidate pair。</p>
<h2 id="数据流挖掘"><a href="#数据流挖掘" class="headerlink" title="数据流挖掘"></a>数据流挖掘</h2><h3 id="数据流挑战"><a href="#数据流挑战" class="headerlink" title="数据流挑战"></a>数据流挑战</h3><p>数据流是无限长度的；是不断进化的；</p>
<ul>
<li>Single Pass Handling （单遍处理）</li>
<li>Memory Limitation （内存限制）</li>
<li>Low Time Complexity （低时间复杂度）</li>
<li>Concept Drift （概念漂移）</li>
</ul>
<h3 id="概念漂移-！！"><a href="#概念漂移-！！" class="headerlink" title="概念漂移[！！]"></a>概念漂移[！！]</h3><p>模型尝试预测的目标变量的某个统计属性随时间发生不可预料的变化。</p>
<p>从本质上说，概念漂移指，数据流的概率分布P(C|X)随时间发生变化。</p>
<p>P(C|X)：在数据条件下，类别的概率分布。</p>
<h3 id="概念漂移的检测方法"><a href="#概念漂移的检测方法" class="headerlink" title="概念漂移的检测方法"></a>概念漂移的检测方法</h3><p>基于分布的检测：ADWIN</p>
<p>基于错误率的检测：PHT</p>
<p>漂移检测方法：DDM</p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>VFDT (Very Fast Decision Tree)</p>
<p>根据Hoeffding Bound利用少量数据构造动态决策树：</p>
<ul>
<li>计算输入数据的所有属性的信息增益，找到增益最大的两个属性A,B。</li>
<li>在每个节点检查两者增益的差G(A)-G(B)若大于Hoeffding Bound，就基于该属性分裂。</li>
<li>如果不大于，继续输入数据，并重新计算增益。</li>
</ul>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>线上+线下</p>
<p>线上：数据抽象，将数据整理成节省空间的数据结构。</p>
<p>线下：用聚类算法找到数据划分</p>
<h4 id="线上："><a href="#线上：" class="headerlink" title="线上："></a>线上：</h4><p>微簇：将非常靠近的一簇数据点当作一个点。</p>
<p>将数据点宏观抽象，以微簇形式维护到内存中，且是<strong>动态变化</strong>的。</p>
<p><strong>簇特征</strong>(Cluster Feature) </p>
<p>CF=(N,LS,SS)</p>
<blockquote>
<p>N：点的个数</p>
<p>LS：（线性）所有行向量的和，即每维（每一列）的数据之和</p>
<p>SS：（平方）所有行向量的平方和</p>
</blockquote>
<p><strong>CF具有可加/减性！！可以实现动态增量计算</strong></p>
<h4 id="线下：Kmeans-DBSCAN"><a href="#线下：Kmeans-DBSCAN" class="headerlink" title="线下：Kmeans/DBSCAN"></a>线下：Kmeans/DBSCAN</h4><h2 id="Hadoop-Spark"><a href="#Hadoop-Spark" class="headerlink" title="Hadoop/Spark"></a>Hadoop/Spark</h2><h3 id="什么是Hadoop-Spark"><a href="#什么是Hadoop-Spark" class="headerlink" title="什么是Hadoop/Spark"></a>什么是Hadoop/Spark</h3><p>Hadoop是一个基于MapReduce的分布式超大数据集处理软件框架。</p>
<blockquote>
<p>关键词：处理大数据，分布式计算，使用大量廉价的商用硬件</p>
</blockquote>
<p>Spark也是分布式计算环境，它启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。它是对 Hadoop 的补充，可以在 Hadoop 文件系统中并行运行。</p>
<blockquote>
<p>但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</p>
</blockquote>
<h3 id="Hadoop设计准则"><a href="#Hadoop设计准则" class="headerlink" title="Hadoop设计准则"></a>Hadoop设计准则</h3><ul>
<li><p><strong>Automatic parallelization &amp; distribution</strong><br>Hidden from the end-user</p>
</li>
<li><p><strong>Fault tolerance and automatic recovery</strong><br>Nodes/tasks will fail and will recover automatically</p>
</li>
<li><p><strong>Clean and simple programming abstraction</strong><br>Users only provide two functions “map” and “reduce”</p>
</li>
</ul>
<h3 id="Hadoop生态"><a href="#Hadoop生态" class="headerlink" title="Hadoop生态"></a>Hadoop生态</h3><p><img src="6422782-fbfd2b834ed3707d.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<h3 id="HDFS-——-存储"><a href="#HDFS-——-存储" class="headerlink" title="HDFS —— 存储[!]"></a>HDFS —— 存储[!]</h3><p>Hadoop Distributed File System</p>
<h4 id="Namenode"><a href="#Namenode" class="headerlink" title="Namenode"></a>Namenode</h4><p>维护文件分块存储信息的元数据。（文件本身默认按64M每block分块。）</p>
<p>Namenode通过心跳机制持续检测Datanode。</p>
<h4 id="Datanode"><a href="#Datanode" class="headerlink" title="Datanode"></a>Datanode</h4><p>保存分块过的文件数据，默认每个block会复制三份保存在不同Datanode上。</p>
<h3 id="MapReduce-——-计算"><a href="#MapReduce-——-计算" class="headerlink" title="MapReduce —— 计算"></a>MapReduce —— 计算</h3><p>Input &gt;&gt; Map Task &gt;&gt; Shuffle&amp;Sort &gt;&gt; Reduce &gt;&gt; Output</p>
<blockquote>
<p>Users only provide the “Map” and “Reduce” functions.</p>
<p>“Shuffle&amp;Sort”相当于自动将所有块映射后的某种键值对合并送到对应reduce函数中。</p>
</blockquote>
<p>Kmeans先在各块中分配簇(map)，再更新中心点(reduce)。</p>
<p>KNN先在各块中找本地近邻点(map)，再从所有本地近邻点中找到全局近邻点(reduce)。</p>
<h3 id="MapReduce-vs-Spark"><a href="#MapReduce-vs-Spark" class="headerlink" title="MapReduce vs Spark[!]"></a>MapReduce vs Spark[!]</h3><p>MapReduce存在缺点：</p>
<ul>
<li><p>适合单遍计算，多遍算法效率低。</p>
</li>
<li><p>使用广播，没有高效率的数据分享方法，多轮IO操作导致很慢。</p>
</li>
</ul>
<p>而Spark的优点：</p>
<ul>
<li><p>使用RDD,<strong>resilient distributed dataset</strong> (弹性分布式数据集)，它是存在内存中的只读数据库，速度快。</p>
</li>
<li><p>Spark提供多个语言的API以供调用。</p>
</li>
</ul>
<h3 id="RDD-操作"><a href="#RDD-操作" class="headerlink" title="RDD 操作"></a>RDD 操作</h3><p><img src="image-20200824144319721.png" srcset="/blogsite/img/loading2.gif" lazyload alt=""></p>
<p>transformations类似map，在已有数据集上得到新的数据集。</p>
<blockquote>
<p>所有transformations都是lazy的，只有需要actions时才进行前置的transformations</p>
</blockquote>
<p>actions类似reduce，汇总分布的数据并处理。</p>
<p>transformations和actions是函数类型的统称，不是具体的某个函数。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/blogsite/categories/%E7%9F%A5%E8%AF%86/">知识</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/blogsite/tags/%E5%A4%8D%E4%B9%A0/">复习</a>
                    
                  </div>
                
              </div>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blogsite/2020/08/28/%E6%B1%AA%E6%B4%8B/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">汪洋</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blogsite/2020/08/02/QQMusic/">
                        <span class="hidden-mobile">QQ音乐API</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/blogsite/js/events.js" ></script>
<script  src="/blogsite/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/blogsite/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/blogsite/js/local-search.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.4/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/blogsite/js/boot.js" ></script>


</body>
</html>
